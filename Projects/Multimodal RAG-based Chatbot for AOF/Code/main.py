import os
import json
import dotenv
import gradio as gr
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

dotenv.load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("Thi·∫øu OpenAI API key. Vui l√≤ng th√™m v√†o file .env.")

IMAGE_FOLDER = "dataset/images"
CAPTION_FILE = "image_captions.json"

with open(CAPTION_FILE, "r", encoding="utf-8") as f:
    image_captions = json.load(f)

vectorstore = Chroma(
    embedding_function=OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
llm = ChatOpenAI(model="gpt-4o-mini")

embed_model = SentenceTransformer("keepitreal/vietnamese-sbert")

def embed(text):
    return embed_model.encode(text)

answer_prompt = PromptTemplate.from_template(
    """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªó tr·ª£ tr·∫£ l·ªùi c√°c c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát, d·ª±a tr√™n n·ªôi dung ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi.

N·ªôi dung tham kh·∫£o:
{context}

C√¢u h·ªèi:
{question}

H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, r√µ r√†ng v√† d·ªÖ hi·ªÉu. N·∫øu c√≥ th·ªÉ, h√£y gi·∫£i th√≠ch th√™m v√† ƒë∆∞a ra v√≠ d·ª• minh h·ªça.

Tr·∫£ l·ªùi:"""
)

summarize_prompt = PromptTemplate.from_template(
    """H√£y vi·∫øt m·ªôt ƒëo·∫°n t√≥m t·∫Øt 1 c√¢u ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát cho n·ªôi dung sau.

{context}

T√≥m t·∫Øt:"""
)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | answer_prompt
    | llm
    | StrOutputParser()
)

summarize_chain = (
    {"context": RunnablePassthrough()}
    | summarize_prompt
    | llm
    | StrOutputParser()
)

def find_related_images(summary_text, image_folder=IMAGE_FOLDER, top_k=2, threshold=0.5):
    answer_vec = embed(summary_text)
    matches = []

    print(f"\n[DEBUG] T√≥m t·∫Øt: {summary_text}")
    print("[DEBUG] ƒêang so s√°nh v·ªõi c√°c caption:")

    for img_file, caption in image_captions.items():
        full_path = os.path.join(image_folder, img_file)
        if not os.path.isfile(full_path):
            print(f"B·ªè qua: {img_file} (file kh√¥ng t·ªìn t·∫°i)")
            continue

        caption_vec = embed(caption)
        sim = cosine_similarity([answer_vec], [caption_vec])[0][0]
        status = "Gi·ªØ l·∫°i" if sim >= threshold else "B·ªè qua"
        print(f"{img_file}: \"{caption}\" ‚Üí Similarity: {sim:.4f} {status}")

        if sim >= threshold:
            matches.append((full_path, sim))

    matches.sort(key=lambda x: x[1], reverse=True)
    print("\nüèÅ [DEBUG] Top ·∫£nh ƒë∆∞·ª£c ch·ªçn:")
    for path, score in matches[:top_k]:
        print(f"  ‚Üí {os.path.basename(path)} (score: {score:.4f})")

    return [m[0] for m in matches[:top_k]]

def answer_question(question):
    if not question.strip():
        return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi.", "", None, None

    try:
        non_rag_answer = llm.invoke(question).content
    except Exception as e:
        non_rag_answer = f"L·ªói GPT: {str(e)}"

    try:
        rag_answer = rag_chain.invoke(question)
        summary = summarize_chain.invoke(rag_answer)
        print("T√≥m t·∫Øt:", summary)
        image_paths = find_related_images(summary)
    except Exception as e:
        rag_answer = f"L·ªói RAG: {str(e)}"
        image_paths = []

    images = [None] * 2
    for i in range(min(len(image_paths), 2)):
        images[i] = image_paths[i]

    return non_rag_answer, rag_answer, *images

with gr.Blocks() as demo:
    gr.Markdown("## Chatbot AOF")

    question_input = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát")
    btn = gr.Button("L·∫•y c√¢u tr·∫£ l·ªùi")

    non_rag_output = gr.Textbox(label="C√¢u tr·∫£ l·ªùi c·ªßa LLM/GPT", lines=4)
    rag_output = gr.Textbox(label="C√¢u tr·∫£ l·ªùi c·ªßa AOF Chatbot", lines=8)

    gr.Markdown("### H√¨nh ·∫£nh li√™n quan")
    with gr.Row():
        image1 = gr.Image(label="·∫¢nh 1", type="filepath")
        image2 = gr.Image(label="·∫¢nh 2", type="filepath")

    btn.click(
        fn=answer_question,
        inputs=question_input,
        outputs=[
            non_rag_output, rag_output,
            image1, image2
        ]
    )

demo.launch(share=True)
